{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1EGdBoM-qWcpbdsteXvjsb2E0hgymOqhK","authorship_tag":"ABX9TyPt3dkVKzJ2hpX1HFElQQLy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Load Best Model and Execute Configurations\n","\n","Before loading the model, ensure to execute the following cell containing the necessary classes and model configurations. This step initializes the required components for the model."],"metadata":{"id":"KQhAVnnlKsFk"}},{"cell_type":"code","source":["!pip install wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fj7yYaULnWL","executionInfo":{"status":"ok","timestamp":1706480850876,"user_tz":-60,"elapsed":29099,"user":{"displayName":"Djillali MAHDID","userId":"05118283520182562452"}},"outputId":"78bb0b63-2db5-415a-aada-83144d075286"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wandb\n","  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n","Successfully installed GitPython-3.1.41 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.2 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.2\n"]}]},{"cell_type":"code","source":["import os\n","import random\n","import tensorflow as tf\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from tensorflow.keras.models import load_model\n","import pickle\n","from tensorflow.keras.preprocessing import image\n","from tensorflow import keras\n","from keras.layers import (\n","    GlobalAveragePooling2D, Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization,\n","    Dropout, Attention, ReLU, Add, Input, Multiply, Layer\n",")\n","from tensorflow.keras.utils import plot_model\n","from keras.models import Model, Sequential\n","from keras.optimizers import Adam, RMSprop, SGD\n","from keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n","import shutil\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.metrics import SparseCategoricalAccuracy\n","import wandb\n","from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n"],"metadata":{"id":"fp4kz0u2LlOO","executionInfo":{"status":"ok","timestamp":1706480859786,"user_tz":-60,"elapsed":8917,"user":{"displayName":"Djillali MAHDID","userId":"05118283520182562452"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Jstrn0IzHbDV","executionInfo":{"status":"ok","timestamp":1706480860289,"user_tz":-60,"elapsed":513,"user":{"displayName":"Djillali MAHDID","userId":"05118283520182562452"}}},"outputs":[],"source":["\n","\n","@keras.saving.register_keras_serializable()\n","class SEBlock(Layer):\n","  \"\"\"\n","    Squeeze-and-Excitation (SE) Block layer.\n","\n","    Args:\n","        channels (int): Number of input channels.\n","        ratio (int): Reduction ratio for the intermediate channels.\n","\n","    Returns:\n","        Output tensor after applying the SE block.\n","  \"\"\"\n","  def __init__(self, channels, ratio= 16):\n","     super(SEBlock, self).__init__()\n","\n","     self.pooling =  GlobalAveragePooling2D()\n","     self.fc1 = Dense(channels // ratio, activation='relu')\n","     self.fc2 = Dense(channels, activation='sigmoid')\n","\n","     def call(sekf, inputs):\n","      output = self.pooling(inputs)\n","      output = self.fc1\n","      output = self.fc2\n","      return output\n","\n","@keras.saving.register_keras_serializable()\n","class SEResBlock(Layer):\n","    \"\"\"\n","    Forward pass of the SE Block layer.\n","\n","    Args:\n","        inputs: Input tensor.\n","\n","    Returns:\n","        Output tensor after applying the SE block.\n","    \"\"\"\n","    def __init__(self, filters):\n","        super(SEResBlock, self).__init__()\n","\n","        # First convolution block in residual\n","        self.conv1 = Conv2D(filters, (3, 3), padding='same', strides=1)\n","        self.bn1 = BatchNormalization()\n","        self.relu1 = ReLU()\n","\n","        # Second convolution block in residual\n","        self.conv2 = Conv2D(filters, (3, 3), padding='same', strides=1)\n","        self.bn2 = BatchNormalization()\n","\n","        # SE Block\n","        self.se_block = SEBlock(filters)\n","\n","        # Shortcut connection\n","        self.shortcut = Conv2D(filters, (1, 1), padding='same', strides=1)\n","        self.add = Add()\n","        self.relu2 = ReLU()\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        Forward pass of the SERes Block layer.\n","\n","        Args:\n","            inputs: Input tensor.\n","\n","        Returns:\n","            Output tensor after applying the SERes block.\n","        \"\"\"\n","        x = self.conv1(inputs)\n","        x = self.bn1(x)\n","        x = self.relu1(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","\n","        # Apply SE Block\n","        se_output = self.se_block(x)\n","        x = Multiply()([x, se_output])\n","\n","        shortcut = self.shortcut(inputs)\n","        x = self.add([x, shortcut])\n","        x = self.relu2(x)\n","\n","        return x\n","\n","\n","@keras.saving.register_keras_serializable()\n","class SEResShallowV1(tf.keras.Model):\n","    \"\"\"\n","    Shallow ResNet model with Squeeze-and-Excitation (SE) blocks.\n","\n","    Args:\n","        num_classes (int): Number of output classes.\n","        filters (int): Number of filters in the initial convolutional layer.\n","        num_blocks (int): Number of residual blocks.\n","\n","    Returns:\n","        Output tensor representing class probabilities.\n","    \"\"\"\n","    def __init__(self, num_classes=5, filters=32, num_blocks=2):\n","        super(SEResShallowV1, self).__init__()\n","\n","        # Initial convolution block\n","        self.conv1 = Conv2D(filters, (3, 3), padding='same', strides=1)\n","        self.bn1 = BatchNormalization()\n","        self.relu1 = ReLU()\n","\n","        # Residual blocks\n","        self.res_blocks = [SEResBlock(filters*pow(2, i)) for i in range(num_blocks)]\n","\n","        # Global average pooling and final dense layer\n","        self.global_avg_pooling = GlobalAveragePooling2D()\n","        self.fc = Dense(num_classes, activation='softmax')\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        Forward pass of the SEResShallowV1 model.\n","\n","        Args:\n","            inputs: Input tensor.\n","\n","        Returns:\n","            Output tensor representing class probabilities.\n","        \"\"\"\n","        x = self.conv1(inputs)\n","        x = self.bn1(x)\n","        x = self.relu1(x)\n","        for res_block in self.res_blocks:\n","            x = res_block(x)\n","\n","        x = self.global_avg_pooling(x)\n","        x = self.fc(x)\n","\n","        return x\n","\n","def create_directory_or_file(path, is_dir=True):\n","    \"\"\"\n","    Create a directory or file at the specified path.\n","\n","    Args:\n","        path (str): Path to the directory or file.\n","        is_dir (bool): If True, create a directory. If False, create a file.\n","\n","    Returns:\n","        bool: True if the directory or file already exists, False if it was created.\n","    \"\"\"\n","    if is_dir:\n","        # Check if the directory exists\n","        if not os.path.exists(path) or not os.path.isdir(path):\n","            os.makedirs(path)\n","            print(f\"The directory '{path}' has been created.\")\n","            return False\n","        return True\n","    else:\n","        # Check if the file exists\n","        if not os.path.exists(path):\n","            # Create the file if it doesn't exist\n","            with open(path, 'w') as file:\n","                pass\n","            print(f\"The file '{path}' has been created.\")\n","            return False\n","        return True\n","\n","\n","\n","def save_checkpoint(path, history):\n","  \"\"\" The output directory structure\n","  output :\n","    - model_directory\n","      1. checkpoints\n","        * model_checkpoint_epoch{number}\n","        ...\n","      2. history.pkl\n","      3. configs.json\n","\n","  \"\"\"\n","  # create checkpoint path if it doesn't exist\n","  full_path = os.path.join(path, \"checkpoints\")\n","  create_directory_or_file(full_path)\n","\n","\n","  # save current evaluation metrics and loss in history.pkl (create it if it doesn't exist)\n","  history_path = os.path.join(path, \"history.pkl\")\n","  res = create_directory_or_file(history_path, dir = False)\n","\n","  # if the file doesn't exist, we fill it with\n","  data = dict()\n","  if not res :\n","    index = 0\n","    data[index] = history\n","\n","    with open(history_path, 'wb') as file:\n","      pickle.dump(data, file)\n","\n","  else:\n","    try:\n","      # Open the file for reading in binary mode\n","      with open(history_path, 'rb+') as file:\n","        # Load the pickled data from the file\n","        loaded_data = pickle.load(file)\n","\n","        # Add the new data inside\n","        index = max(loaded_data.keys())+1\n","        loaded_data[index] = history\n","\n","      with open(history_path, 'wb') as file:\n","        # Save the new content in the file\n","        pickle.dump(loaded_data, file)\n","\n","    except FileNotFoundError:\n","        print(f\"The file '{history_path}' does not exist.\")\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","\n","\n","\n","\n","class SaveCallback(Callback):\n","    \"\"\"\n","    Callback to save model checkpoints and logs at the end of each epoch.\n","\n","    Args:\n","        save_path (str): Path to the directory where checkpoints and logs will be saved.\n","    \"\"\"\n","    def __init__(self, save_path):\n","        super(SaveCallback, self).__init__()\n","        self.save_path = save_path\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        # Access the current loss and accuracy\n","        save_checkpoint(path = self.save_path, history=logs)\n","        wandb.log(logs)\n","\n","# Define model configurations\n","optimizer_configs = {\"adam\" : {\n","                        \"learning_rate\": 0.0005,\n","                        \"beta_1\": 0.9,\n","                        \"beta_2\": 0.999,\n","                        \"epsilon\": 1e-07\n","                      },\n","\n","                     \"rms_prop\": {\n","                         \"learning_rate\": 0.001,\n","                         \"rho\": 0.9,\n","                         \"momentum\": 0.0,\n","                         \"epsilon\": 1e-07\n","                     },\n","\n","                     \"sgd\": {\n","                         \"learning_rate\": 0.01,\n","                         \"momentum\": 0.0\n","                     }\n","                  }\n","\n","loss_configs = {\"sparse_categorical_crossentropy\":{\n","               \"from_logits\": False}}\n","\n","\n","# Define compiling configurations\n","model_configs = {\"optimizers\": optimizer_configs,\n","           \"losses\": loss_configs}\n","\n"]},{"cell_type":"markdown","source":["## Generate CSV File\n","\n","This function takes the path to your test data as input and generates a CSV file as output. The CSV file includes the following columns:\n","\n","- `image_name`: Name of the image file.\n","- `relative_path`: Relative path to the image file within the provided test data directory.\n","- `class_label`: Class label assigned to the image.\n","\n","To use this function, provide the path to your test data directory, and the output CSV file will be created with the specified columns. The CSV file will contain information about each image, making it suitable for further analysis or evaluation.\n"],"metadata":{"id":"BohWftg0I6WP"}},{"cell_type":"code","source":["\n","\n","# Test images in the directory\n","def generate_csv_file(directory_path,model_path, random_seed=None, output_csv_path='output.csv'):\n","    if random_seed is not None:\n","        random.seed(random_seed)\n","\n","    all_images = [filename for filename in os.listdir(directory_path) if filename.endswith(('.jpg', '.jpeg', '.png'))]\n","\n","    predictions = []\n","    image_names = []\n","    relative_paths = []\n","    # Open the file for reading in binary mode\n","\n","\n","    model = tf.keras.models.load_model(model_path, custom_objects={\"se_block\": SEBlock, \"se_res_block\": SEResBlock,\"se_res_shallowv1\": SEResShallowV1})\n","\n","\n","    for filename in all_images:\n","        image_path = os.path.join(directory_path, filename)\n","        relative_path = os.path.relpath(image_path, directory_path)\n","\n","        # You may need to adjust this part based on your model input requirements\n","        # Read the image file and perform minimal processing\n","        imag = image.load_img(image_path, target_size=(28, 28), color_mode='grayscale')\n","        imag_arr = image.img_to_array(imag)\n","        imag_arr = np.expand_dims(imag_arr, axis=0)\n","        imag = imag_arr / 255.0  # Normalize if needed\n","\n","\n","        # Make predictions using your model\n","        prediction = model.predict(imag)\n","        class_label = np.argmax(prediction)\n","\n","        predictions.append(class_label)\n","        image_names.append(filename)\n","        relative_paths.append(relative_path)\n","\n","    df = pd.DataFrame({'image_name': image_names, 'relative_path': relative_paths, 'class_label': predictions})\n","    df.to_csv(output_csv_path, index=False)\n","    print(f'CSV file saved at {output_csv_path}')\n"],"metadata":{"id":"UFQDzDVzIRj4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Change this path according to your model.keras's directory\n","model_path = \"/content/drive/MyDrive/ISIMA2/ISIMA/Deep learning/Challenge/Challenge/livrable/model.keras\"\n","# Change this path according to your test_data's directory\n","directory_path = '/content/drive/MyDrive/ISIMA2/ISIMA/Deep learning/Challenge/Challenge/test_data'\n","\n","\n","generate_csv_file(directory_path, model_path)"],"metadata":{"id":"AUkSy6WcNtll"},"execution_count":null,"outputs":[]}]}